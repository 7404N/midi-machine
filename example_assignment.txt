INTRODUCTION
------------
The purpose of this assignment is to create learning machines that can generate
music in the style of the training data.

Provided is a framework for training and testing various network architectures
using any MIDI data as input. We suggest using the jsb Chorales data.

To get a list of time signatures, run the following command from the
./lua directory of the package:

  $ th -e 'require "mid"; print(mid.dataset.stat("../midi"))'

The output shows every time signature and the number of example points in the
dataset. Note that the errors of the form

  Failed to process ../**/*.mid
  ./mid/dataset.lua:117: Time signature set at delta != 0

may be ignored. These files are not included in processing because they contain
time signature MIDI events after the start of the data.

See the README for more information about time signatures.

WHAT YOU GET
------------
The lua/models.lua module has 3 types of feed-forward neural networks that it
can generate. Any feed-forward network is compatible with the nn.Rnn
(lua/Rnn.lua) module to create a Recurrent Neural Network.

Out of the box, the learner will generate very simple sequences of notes. For
an input of 10 timesteps, the learner can generate about 20 timesteps before
the output diminishes to near silence.

QUESTIONS
---------
1) Explore.
   Explore the relationship between the number of hidden units and the output.
   Try different time signatures to get different datasets. Try training with
   subsets of the data to speed up your experiments. Try different input
   sequence lengths and RNN unrollings. You may want to write some code that
   can run several smaller jobs as once.

   Summarize your findings:
   * What qualitative impacts do you find from different numbers of hidden
     units? Submit at least 3 MIDI files generated by the same family of model
     and document the models used and their differences.

   * What difference does the RNN make on the output? Submit at least 2 MIDI
     files demonstrating some results with a standard feed-forward network and
     an RNN. Explain the differences.

   * How much data do you need to train a model? How do you know that you're
     getting good performance given the amount of data that you used? Which
     time signatures work best? Which ones the worst? Submit at least 3 MIDI
     files demonstrating the same time signature with different training sample
     sizes. Submit at least 2 MIDI files showing the best and worst time
     signatures. Try to explain why the worst time signature is particularly
     difficult to predict.

2) Reconstruction.
   The data format provided loses something in the conversion from MIDI events
   to a rasterized matrix. In particular, it is not possible to release and
   engage the same note on the same time step at the same velocity, since there
   is no difference between a strike and a hold at the same velocity. For
   example, the input

      play note 12 from time 3 until time 7 at velocity 64
      play note 12 from time 7 until time 22 at velocity 64

   will appear in the output format as a solid activation of note 12 at level
   64 from time 3 until time 22. Analyze a dataset from your favorite time
   signature to determine whether or not this is an issue worth correcting.
   Hint: you will need to use the format provided by mid.data.read() do perform
   this analysis. Note that these are all stored in ds.sources[i].middata, so
   you can use mid.dataset.load() as usual to perform the analysis.

   If your analysis reveals that this these data are important, then propose a
   format for the rasterized output that does not lose this information. By
   what order of magnitude do the data grow? How might you change your network
   architecture to process this signal?

3) Regularization.
   The networks provided do not have any regularization. Experiment with one of
   { L1, L2, Dropout }. Compare the difference in output. Did you expect the
   change that you hear? Why or why not? Provide at least 2 midi files showing
   the difference between regularized and non-regularized networks.

4) Network architectures.
   Experiment with some new network architectures. Some possibilities include
   deeper simple networks, convnets, and recurrent convnets. For each
   experiment, talk about the design a bit any why you think it could improve
   the result. What kinds of higher-level features might you capture? What
   dimensions does your network use to extract features?

   Provide at least 3 MIDI files and 3 network architectures not included in
   the original codebase.

5) Modelling dynamics.
   Experiment with adding hidden states to your machine that are recurrent
   within the network.
   
   At an internal layer of your choosing, inject some hidden state of a
   dimension of your choosing by concatenating some vector with the data
   representation at the chosen layer. For instance, suppose that the internal
   data have a matrix representation of size 10x15 at some layer L_3. You could
   concatenate some hidden state of size 3x15 to give an augmented state of
   size 13x15 at L_3. Then, at some layer L_K with K > 3, extract some state
   whose nElements() == 3x15 = 45. Recur this processed hidden state into the
   network at L_3 on the next unrolling. Note that you will need to initialize
   randomly the hidden state whenever you run your network in feed-forward mode
   to generate a song.

   The idea is to have some higher-level state that is a sort of dynamical
   memory for the machine. You could think of this hidden state as the mood of
   the machine.

   Observe what happens when adding this state. Report finding using at least 3
   MIDI files. We recommend that you begin with very simple hidden states that
   do not propagate very deeply at first, then increase complexity only after
   observing some interesting result.

6) Loss functions.
   The MSE works very well in this instance. Experiment with some other loss
   functions. Submit at least 3 MIDI files with different types of loss
   functions. If they seem improved, state why you think so. If they sound
   completely wrong, state why you think so. Are there any properties of music
   or hearing that might be worth encoding in the loss function?
